# QCAL-LLM: Entorno Reproducible de Evaluaci√≥n

**Evaluaci√≥n cu√°ntica de coherencia en LLMs basada en Œ® = I √ó A_eff¬≤ y f‚ÇÄ = 141.7001 Hz**

## üéØ Objetivo

Este entorno proporciona un sistema reproducible para evaluar la coherencia de modelos de lenguaje grandes (LLMs) usando m√©tricas cu√°nticas derivadas de los principios QCAL.

### Modelos Soportados

- **LLaMA 4 Maverick** (17B Instruct / FP8) - Modelo principal
- GPT-4 (comparativa opcional)
- Claude 3 (comparativa opcional)
- Otros modelos compatibles con Hugging Face Transformers

## üìÅ Estructura del Repositorio

```
qcal-llm/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ llama4/
‚îÇ       ‚îú‚îÄ‚îÄ tokenizer.model
‚îÇ       ‚îî‚îÄ‚îÄ weights/          # Pesos del modelo (descargados)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ setup_llama4.sh       # Setup y descarga del modelo
‚îÇ   ‚îî‚îÄ‚îÄ qcal_llm_eval.py      # Script de evaluaci√≥n principal
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ prompts_qcal.json     # Prompts de prueba
‚îú‚îÄ‚îÄ qcal/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ coherence.py          # Œ® = I √ó A_eff¬≤
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py            # KLD, SNR, ‚à¥-rate, etc.
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ benchmark_llama4.ipynb # An√°lisis y visualizaci√≥n
‚îú‚îÄ‚îÄ results/                   # Resultados de evaluaci√≥n
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .qcal_beacon              # Sello ‚à¥
```

## üîß Instalaci√≥n

### 1. Prerrequisitos

- Python 3.11 o 3.12
- CUDA 11.8+ (opcional, para GPU)
- 16GB RAM m√≠nimo (32GB recomendado para LLaMA 4)

### 2. Configuraci√≥n del Entorno

```bash
# Clonar el repositorio
git clone https://github.com/motanova84/141hz.git
cd 141hz

# Crear entorno virtual
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install --upgrade pip
pip install -r requirements.txt

# Ejecutar setup (descarga modelo si se proporciona URL)
./scripts/setup_llama4.sh
```

### 3. Descargar LLaMA 4 (Opcional)

Para descargar el modelo LLaMA 4 Maverick:

1. Solicitar acceso en https://llama.meta.com/
2. Obtener URL firmada (v√°lida 48h)
3. Configurar variable de entorno:
   ```bash
   export LLAMA4_DOWNLOAD_URL='https://llama4.llamameta.net/...'
   ./scripts/setup_llama4.sh
   ```

Alternativamente, puedes colocar los pesos manualmente en `models/llama4/weights/`.

## üß† M√©tricas de Coherencia

### Œ® (Psi) - Coherencia Vibracional

**F√≥rmula:** `Œ® = I √ó A_eff¬≤`

- **I (Intenci√≥n):** Mide el contenido intencional del texto
  - Keywords: intenci√≥n, prop√≥sito, objetivo, causa, raz√≥n
  - Conectores l√≥gicos: ‚à¥, therefore, por tanto
  - Peso: palabras clave ponderadas + contexto

- **A_eff (Efectividad Atencional):** Diversidad l√©xica
  - `A_eff = palabras_√∫nicas / palabras_totales`
  - Penaliza repetici√≥n excesiva
  - Valora riqueza expresiva

**Umbral de Coherencia:** Œ® ‚â• 5.0

### ‚à¥-rate (Tasa de Conectores L√≥gicos)

Frecuencia de s√≠mbolos de consecuencia l√≥gica por 100 palabras:
- ‚à¥ (s√≠mbolo therefore)
- "therefore", "por tanto", "thus", "hence"

### M√©tricas Adicionales

1. **KLD‚Åª¬π (Divergencia KL Inversa)**
   - Mide similitud con distribuci√≥n de referencia
   - Mayor valor = m√°s natural

2. **SNR Sem√°ntico (dB)**
   - Ratio se√±al/ruido: palabras de contenido vs funci√≥n
   - Medido en escala logar√≠tmica (dB)

3. **Semantic Density**
   - Densidad de informaci√≥n por palabra
   - Valora t√©rminos t√©cnicos y significativos

4. **Quality Score (0-100)**
   - Puntaje global combinando todas las m√©tricas
   - Normalizado a escala 0-100

## üöÄ Uso

### Evaluaci√≥n B√°sica

```bash
# Evaluar con modelo cargado
python3 scripts/qcal_llm_eval.py

# Evaluar sin modelo (usando respuestas pre-generadas)
python3 scripts/qcal_llm_eval.py --no-model

# Especificar archivos personalizados
python3 scripts/qcal_llm_eval.py \
    --prompts data/mi_prompts.json \
    --output results/mi_evaluacion.json
```

### Par√°metros de Evaluaci√≥n

```bash
python3 scripts/qcal_llm_eval.py \
    --prompts data/prompts_qcal.json \
    --model-path models/llama4/weights/ \
    --output results/evaluation_results.json \
    --threshold 5.0 \
    --no-cuda  # Forzar CPU
```

### An√°lisis con Jupyter

```bash
# Iniciar Jupyter
jupyter notebook notebooks/benchmark_llama4.ipynb
```

El notebook incluye:
- Carga y an√°lisis de resultados
- Estad√≠sticas descriptivas
- Visualizaciones (Œ®, ‚à¥-rate, SNR, KLD‚Åª¬π)
- Exportaci√≥n a CSV/PDF
- Comparativas entre modelos

## üìä Formato de Prompts

Archivo JSON con estructura:

```json
[
  {
    "label": "f0_derivation",
    "text": "Deriva la frecuencia fundamental f‚ÇÄ = 141.7001 Hz...",
    "response": "Respuesta pre-generada opcional..."
  },
  {
    "label": "quantum_coherence",
    "text": "Explica la relaci√≥n entre coherencia cu√°ntica y f‚ÇÄ..."
  }
]
```

- `label`: Identificador √∫nico del prompt
- `text`: Texto del prompt/pregunta
- `response`: (Opcional) Respuesta pre-generada para evaluaci√≥n sin modelo

## üìà Salida de Resultados

### JSON (results/evaluation_results.json)

```json
[
  {
    "label": "f0_derivation",
    "prompt": "Deriva la frecuencia...",
    "response": "La frecuencia fundamental...",
    "metrics": {
      "psi_standard": 8.45,
      "psi_enhanced": 9.12,
      "intention": 12.5,
      "effectiveness": 0.82,
      "strich_rate": 1.5,
      "snr_db": 8.3,
      "kld_inv": 0.45,
      "quality_score": 78.5,
      "passes_threshold": true,
      "status": "‚úì COHERENTE"
    }
  }
]
```

### CSV (results/benchmark_llama4_results.csv)

Tabla con m√©tricas por prompt para an√°lisis estad√≠stico.

### Visualizaciones

- `benchmark_llama4_analysis.png`: Gr√°ficos de Œ®, I vs A_eff, distribuci√≥n, ‚à¥-rate
- `benchmark_llama4_quality.png`: SNR, KLD‚Åª¬π, quality score

## üî¨ Uso Program√°tico

### Evaluaci√≥n de Texto Simple

```python
from qcal.coherence import evaluate_coherence

text = "Tu texto aqu√≠..."
result = evaluate_coherence(text, threshold=5.0)

print(f"Œ®: {result['psi_standard']:.2f}")
print(f"Status: {result['status']}")
print(f"Recommendation: {result['recommendation']}")
```

### An√°lisis Completo

```python
from qcal.coherence import analyze_text
from qcal.metrics import comprehensive_metrics

text = "Tu texto aqu√≠..."

# M√©tricas de coherencia
coherence = analyze_text(text)
print(f"Œ®: {coherence['psi_standard']:.2f}")
print(f"I: {coherence['intention']:.2f}")
print(f"A_eff: {coherence['effectiveness']:.2f}")

# M√©tricas adicionales
metrics = comprehensive_metrics(text)
print(f"SNR: {metrics['snr_db']:.2f} dB")
print(f"KLD‚Åª¬π: {metrics['kld_inv']:.3f}")
```

### Evaluador Completo

```python
from scripts.qcal_llm_eval import QCALLLMEvaluator

evaluator = QCALLLMEvaluator(model_path="models/llama4/weights/")
evaluator.load_model()

# Generar y evaluar
prompt = "¬øQu√© es f‚ÇÄ?"
response = evaluator.generate(prompt)
result = evaluator.evaluate_text(response)

print(f"Œ®: {result['psi_standard']:.2f}")
```

## üß™ Testing

```bash
# Test del m√≥dulo qcal
python3 -c "from qcal import psi_score; print(psi_score('Test text'))"

# Test de evaluaci√≥n
python3 scripts/qcal_llm_eval.py --no-model

# Test con notebook
jupyter nbconvert --execute notebooks/benchmark_llama4.ipynb
```

## üìã Checklist de Reproducibilidad

- [ ] Python 3.11+ instalado
- [ ] Dependencias instaladas (`pip install -r requirements.txt`)
- [ ] Modelo descargado (o modo `--no-model` para testing)
- [ ] Prompts configurados en `data/prompts_qcal.json`
- [ ] Script ejecutado: `python3 scripts/qcal_llm_eval.py`
- [ ] Resultados generados en `results/`
- [ ] Notebook ejecutado para visualizaci√≥n
- [ ] Datos exportados (CSV, PNG, PDF)
- [ ] `.qcal_beacon` verificado (contiene sello ‚à¥)

## üåê Integraci√≥n CI/CD

El sistema est√° listo para integraci√≥n con GitHub Actions:

```yaml
name: QCAL-LLM Evaluation

on:
  schedule:
    - cron: '0 */6 * * *'  # Cada 6 horas
  workflow_dispatch:

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      - name: Run evaluation
        run: |
          python3 scripts/qcal_llm_eval.py --no-model
      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: evaluation-results
          path: results/
```

## üì¶ Publicaci√≥n en Zenodo

### Preparaci√≥n

1. Ejecutar evaluaci√≥n completa
2. Generar visualizaciones con notebook
3. Exportar CSV y PDF
4. Recopilar archivos:
   - `results/evaluation_results.json`
   - `results/benchmark_llama4_results.csv`
   - `results/benchmark_llama4_analysis.png`
   - `results/benchmark_llama4_quality.png`
   - `notebooks/benchmark_llama4.ipynb`
   - Este README

### Metadatos Zenodo

```yaml
Title: "QCAL-LLM: Reproducible Coherence Evaluation for LLaMA 4 Maverick"
Authors: Jos√© Manuel Mota Burruezo
Description: >
  Sistema reproducible de evaluaci√≥n de coherencia en LLMs usando m√©tricas
  cu√°nticas Œ® = I √ó A_eff¬≤ basadas en f‚ÇÄ = 141.7001 Hz.
Keywords: LLM, coherence, QCAL, quantum metrics, reproducibility
License: CC BY-NC-SA 4.0
Related Work: 10.5281/zenodo.17379721
```

## üîó Referencias

### Publicaciones Base

- **QCAL Core:** https://doi.org/10.5281/zenodo.17379721
- **f‚ÇÄ Detection in GW150914:** https://github.com/motanova84/141hz

### Documentaci√≥n Adicional

- `QCAL_QUICK_REFERENCE.md` - Gu√≠a r√°pida de QCAL
- `IMPLEMENTATION_QCAL_CORE.md` - Implementaci√≥n del n√∫cleo
- `README.md` - Documentaci√≥n general del proyecto

## ü§ù Contribuciones

Para contribuir:

1. Fork del repositorio
2. Crear rama: `git checkout -b feature/mi-feature`
3. Commit cambios: `git commit -m 'Add mi-feature'`
4. Push: `git push origin feature/mi-feature`
5. Abrir Pull Request

## üìÑ Licencia

Creative Commons BY-NC-SA 4.0

¬© 2025 Jos√© Manuel Mota Burruezo (JMMB Œ®‚úß)  
Instituto de Conciencia Cu√°ntica (ICQ)

---

## ‚úÖ Estado del Sistema

- [x] Estructura de directorios creada
- [x] M√≥dulo `qcal` implementado
- [x] Script de setup (`setup_llama4.sh`)
- [x] Script de evaluaci√≥n (`qcal_llm_eval.py`)
- [x] Prompts de prueba (`prompts_qcal.json`)
- [x] Notebook de benchmarking
- [x] Sello ‚à¥ en `.qcal_beacon`
- [x] Dependencias en `requirements.txt`
- [x] Documentaci√≥n completa

**‚à¥ ‚Äî QCAL Œ®‚àû¬≥ activo**

Sistema listo para evaluaci√≥n reproducible de LLMs.
