{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCAL-LLM: Benchmark LLaMA 4 Maverick\n",
    "\n",
    "Evaluaci√≥n de coherencia usando Œ® = I √ó A_eff¬≤ y f‚ÇÄ = 141.7001 Hz\n",
    "\n",
    "**Modelos evaluados:**\n",
    "- LLaMA 4 Maverick (17B Instruct / FP8)\n",
    "- GPT-4 (comparativa opcional)\n",
    "- Claude 3 (comparativa opcional)\n",
    "\n",
    "**M√©tricas:**\n",
    "- Œ® (coherencia vibracional)\n",
    "- ‚à¥-rate (tasa de conectores l√≥gicos)\n",
    "- SNR sem√°ntico\n",
    "- KLD‚Åª¬π (divergencia inversa)\n",
    "- Calidad global (0-100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from qcal.coherence import psi_score, analyze_text, evaluate_coherence\n",
    "from qcal.metrics import comprehensive_metrics, quality_score\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "print(f\"üìä Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Evaluation Results\n",
    "\n",
    "Load results from the evaluation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_file = Path('../results/evaluation_results.json')\n",
    "\n",
    "if results_file.exists():\n",
    "    with open(results_file, 'r', encoding='utf-8') as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(results)} evaluation results\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results file found. Run: python scripts/qcal_llm_eval.py\")\n",
    "    print(\"Using sample data for demonstration...\")\n",
    "    \n",
    "    # Load prompts and evaluate directly\n",
    "    with open('../data/prompts_qcal.json', 'r', encoding='utf-8') as f:\n",
    "        prompts = json.load(f)\n",
    "    \n",
    "    results = []\n",
    "    for prompt in prompts:\n",
    "        if 'response' in prompt:\n",
    "            metrics = evaluate_coherence(prompt['response'])\n",
    "            results.append({\n",
    "                'label': prompt['label'],\n",
    "                'prompt': prompt['text'],\n",
    "                'response': prompt['response'],\n",
    "                'metrics': metrics\n",
    "            })\n",
    "    print(f\"‚úÖ Evaluated {len(results)} prompts\")\n",
    "\n",
    "# Display first result as example\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example result:\")\n",
    "print(\"=\"*80)\n",
    "example = results[0]\n",
    "print(f\"Label: {example['label']}\")\n",
    "print(f\"Prompt: {example['prompt'][:100]}...\")\n",
    "print(f\"Response: {example['response'][:150]}...\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Œ®: {example['metrics']['psi_standard']:.2f}\")\n",
    "print(f\"  Status: {example['metrics']['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Analysis\n",
    "\n",
    "Calculate summary statistics for all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics into DataFrame\n",
    "data = []\n",
    "for r in results:\n",
    "    row = {\n",
    "        'label': r['label'],\n",
    "        'psi': r['metrics']['psi_standard'],\n",
    "        'intention': r['metrics']['intention'],\n",
    "        'effectiveness': r['metrics']['effectiveness'],\n",
    "        'strich_rate': r['metrics']['strich_rate'],\n",
    "        'coherent': r['metrics']['passes_threshold'],\n",
    "    }\n",
    "    \n",
    "    # Add optional metrics if available\n",
    "    if 'quality_score' in r['metrics']:\n",
    "        row['quality'] = r['metrics']['quality_score']\n",
    "    if 'snr_db' in r['metrics']:\n",
    "        row['snr_db'] = r['metrics']['snr_db']\n",
    "    if 'kld_inv' in r['metrics']:\n",
    "        row['kld_inv'] = r['metrics']['kld_inv']\n",
    "    \n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(df.describe())\n",
    "\n",
    "# Calculate pass rate\n",
    "pass_rate = df['coherent'].sum() / len(df) * 100\n",
    "print(f\"\\n‚úÖ Coherent responses (Œ® ‚â• 5.0): {df['coherent'].sum()}/{len(df)} ({pass_rate:.1f}%)\")\n",
    "\n",
    "# Show per-prompt results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-PROMPT RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(df[['label', 'psi', 'intention', 'effectiveness', 'coherent']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations\n",
    "\n",
    "Create comprehensive visualizations of the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Œ® scores by prompt\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1a. Œ® scores bar chart\n",
    "ax = axes[0, 0]\n",
    "colors = ['green' if c else 'red' for c in df['coherent']]\n",
    "ax.bar(range(len(df)), df['psi'], color=colors, alpha=0.7)\n",
    "ax.axhline(y=5.0, color='orange', linestyle='--', label='Threshold (Œ®=5.0)', linewidth=2)\n",
    "ax.set_xlabel('Prompt')\n",
    "ax.set_ylabel('Œ® Score')\n",
    "ax.set_title('Coherence Scores (Œ® = I √ó A_eff¬≤)')\n",
    "ax.set_xticks(range(len(df)))\n",
    "ax.set_xticklabels(df['label'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 1b. I vs A_eff scatter\n",
    "ax = axes[0, 1]\n",
    "scatter = ax.scatter(df['effectiveness'], df['intention'], \n",
    "                     c=df['psi'], cmap='viridis', s=100, alpha=0.7)\n",
    "ax.set_xlabel('A_eff (Effectiveness)')\n",
    "ax.set_ylabel('I (Intention)')\n",
    "ax.set_title('Intention vs Effectiveness')\n",
    "plt.colorbar(scatter, ax=ax, label='Œ®')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 1c. Distribution of Œ®\n",
    "ax = axes[1, 0]\n",
    "ax.hist(df['psi'], bins=10, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax.axvline(x=5.0, color='orange', linestyle='--', label='Threshold', linewidth=2)\n",
    "ax.set_xlabel('Œ® Score')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Coherence Scores')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 1d. ‚à¥-rate\n",
    "ax = axes[1, 1]\n",
    "ax.bar(range(len(df)), df['strich_rate'], color='purple', alpha=0.7)\n",
    "ax.set_xlabel('Prompt')\n",
    "ax.set_ylabel('‚à¥-rate (per 100 words)')\n",
    "ax.set_title('Logical Connector Rate')\n",
    "ax.set_xticks(range(len(df)))\n",
    "ax.set_xticklabels(df['label'], rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/benchmark_llama4_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Figure saved: results/benchmark_llama4_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Metrics\n",
    "\n",
    "Analyze additional quality metrics (SNR, KLD‚Åª¬π, quality score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if quality metrics are available\n",
    "has_quality = 'quality' in df.columns\n",
    "has_snr = 'snr_db' in df.columns\n",
    "has_kld = 'kld_inv' in df.columns\n",
    "\n",
    "if has_quality or has_snr or has_kld:\n",
    "    n_plots = sum([has_quality, has_snr, has_kld])\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(6*n_plots, 4))\n",
    "    \n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    plot_idx = 0\n",
    "    \n",
    "    if has_quality:\n",
    "        ax = axes[plot_idx]\n",
    "        ax.bar(range(len(df)), df['quality'], color='teal', alpha=0.7)\n",
    "        ax.set_xlabel('Prompt')\n",
    "        ax.set_ylabel('Quality Score')\n",
    "        ax.set_title('Overall Quality (0-100)')\n",
    "        ax.set_xticks(range(len(df)))\n",
    "        ax.set_xticklabels(df['label'], rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plot_idx += 1\n",
    "    \n",
    "    if has_snr:\n",
    "        ax = axes[plot_idx]\n",
    "        ax.bar(range(len(df)), df['snr_db'], color='orange', alpha=0.7)\n",
    "        ax.set_xlabel('Prompt')\n",
    "        ax.set_ylabel('SNR (dB)')\n",
    "        ax.set_title('Semantic Signal-to-Noise Ratio')\n",
    "        ax.set_xticks(range(len(df)))\n",
    "        ax.set_xticklabels(df['label'], rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plot_idx += 1\n",
    "    \n",
    "    if has_kld:\n",
    "        ax = axes[plot_idx]\n",
    "        ax.bar(range(len(df)), df['kld_inv'], color='green', alpha=0.7)\n",
    "        ax.set_xlabel('Prompt')\n",
    "        ax.set_ylabel('KLD‚Åª¬π')\n",
    "        ax.set_title('Inverse KL Divergence')\n",
    "        ax.set_xticks(range(len(df)))\n",
    "        ax.set_xticklabels(df['label'], rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/benchmark_llama4_quality.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\n‚úÖ Figure saved: results/benchmark_llama4_quality.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Quality metrics not available in results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results\n",
    "\n",
    "Export results to CSV and PDF for publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "csv_file = '../results/benchmark_llama4_results.csv'\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"\\n‚úÖ Results exported to: {csv_file}\")\n",
    "\n",
    "# Create summary report\n",
    "summary = {\n",
    "    'model': 'LLaMA 4 Maverick (17B Instruct / FP8)',\n",
    "    'total_prompts': len(df),\n",
    "    'coherent_count': int(df['coherent'].sum()),\n",
    "    'coherent_rate': float(df['coherent'].sum() / len(df)),\n",
    "    'psi_mean': float(df['psi'].mean()),\n",
    "    'psi_std': float(df['psi'].std()),\n",
    "    'psi_min': float(df['psi'].min()),\n",
    "    'psi_max': float(df['psi'].max()),\n",
    "    'intention_mean': float(df['intention'].mean()),\n",
    "    'effectiveness_mean': float(df['effectiveness'].mean()),\n",
    "    'strich_rate_mean': float(df['strich_rate'].mean()),\n",
    "}\n",
    "\n",
    "if has_quality:\n",
    "    summary['quality_mean'] = float(df['quality'].mean())\n",
    "if has_snr:\n",
    "    summary['snr_mean_db'] = float(df['snr_db'].mean())\n",
    "if has_kld:\n",
    "    summary['kld_inv_mean'] = float(df['kld_inv'].mean())\n",
    "\n",
    "# Save summary\n",
    "summary_file = '../results/benchmark_llama4_summary.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Summary exported to: {summary_file}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key:25s}: {value}\")\n",
    "\n",
    "print(\"\\n‚à¥ ‚Äî QCAL Œ®‚àû¬≥\")\n",
    "print(\"Benchmark complete. Ready for Zenodo publication.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with Other Models (Optional)\n",
    "\n",
    "If you have results from GPT-4, Claude, etc., load and compare them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load comparison data\n",
    "# gpt4_results = json.load(open('../results/gpt4_results.json'))\n",
    "# claude_results = json.load(open('../results/claude_results.json'))\n",
    "\n",
    "# Create comparison DataFrame and visualizations\n",
    "# ...\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Comparison with other models not yet implemented.\")\n",
    "print(\"To compare: Run evaluation with different models and load results here.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
